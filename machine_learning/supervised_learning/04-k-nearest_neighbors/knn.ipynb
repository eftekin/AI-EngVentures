{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Between Points - 2D\n",
    "\n",
    "This topic explains how to calculate the distance between two points in a two-dimensional space using the Distance Formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "    length_diffrence = (movie1[0] - movie2[0]) ** 2\n",
    "    year_difference = (movie1[1] - movie2[1]) ** 2\n",
    "    distance = (length_diffrence + year_difference) ** 0.5\n",
    "    return distance\n",
    "\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Between Points - 3D\n",
    "\n",
    "This topic extends the distance formula to three dimensions, adding a third feature like a movie's budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = [125, 1977, 11000000]\n",
    "raiders = [115, 1981, 18000000]\n",
    "mean_girls = [97, 2004, 17000000]\n",
    "\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "    squared_difference = 0\n",
    "    for i in range(len(movie1)):\n",
    "        squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "    final_distance = squared_difference**0.5\n",
    "    return final_distance\n",
    "\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data with Different Scales\n",
    "\n",
    "### Normalization\n",
    "\n",
    "When features in a dataset have vastly different scales (e.g., release date vs. budget), it skews the distance calculations. To address this, we normalize the data, typically using min-max normalization, to bring all values between 0 and 1. This ensures that no single feature disproportionately influences the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dates = [\n",
    "    1897.0,\n",
    "    1998.0,\n",
    "    2000.0,\n",
    "    1948.0,\n",
    "    1962.0,\n",
    "    1950.0,\n",
    "    1975.0,\n",
    "    1960.0,\n",
    "    2017.0,\n",
    "    1937.0,\n",
    "    1968.0,\n",
    "    1996.0,\n",
    "    1944.0,\n",
    "    1891.0,\n",
    "    1995.0,\n",
    "    1948.0,\n",
    "    2011.0,\n",
    "    1965.0,\n",
    "    1891.0,\n",
    "    1978.0,\n",
    "]\n",
    "\n",
    "\n",
    "def min_max_normalize(lst):\n",
    "    minimum = min(lst)\n",
    "    maximum = max(lst)\n",
    "    normalized = []\n",
    "\n",
    "    for value in lst:\n",
    "        normalized_num = (value - minimum) / (maximum - minimum)\n",
    "        normalized.append(normalized_num)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "print(min_max_normalize(release_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Nearest Neighbors\n",
    "\n",
    "After normalizing the data, we find the k nearest neighbors for an unclassified point by calculating the distance between it and all other points in the dataset. We then sort these distances and choose the k closest neighbors to classify the new point. For example, if k is 5, we select the 5 movies with the smallest distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(movie1, movie2):\n",
    "    squared_difference = 0\n",
    "    for i in range(len(movie1)):\n",
    "        squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "    final_distance = squared_difference**0.5\n",
    "    return final_distance\n",
    "\n",
    "\n",
    "def classify(unknown, dataset, k):\n",
    "    distances = []\n",
    "    # Looping through all points in the dataset\n",
    "    for title in dataset:\n",
    "        movie = dataset[title]\n",
    "        distance_to_point = distance(movie, unknown)\n",
    "        # Adding the distance and point associated with that distance\n",
    "        distances.append([distance_to_point, title])\n",
    "    distances.sort()\n",
    "    # Taking only the k closest points\n",
    "    neighbors = distances[0:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Neighbors\n",
    "\n",
    "After finding the k nearest neighbors, we classify the unclassified point by counting how many neighbors are labeled as \"good\" or \"bad.\" The classification will be based on the majority. In case of a tie, one strategy is to assign the label of the closest neighbor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(unknown, dataset, labels, k):\n",
    "    distances = []\n",
    "    # Looping through all points in the dataset\n",
    "    for title in dataset:\n",
    "        movie = dataset[title]\n",
    "        distance_to_point = distance(movie, unknown)\n",
    "        # Adding the distance and point associated with that distance\n",
    "        distances.append([distance_to_point, title])\n",
    "    distances.sort()\n",
    "    # Taking only the k closest points\n",
    "    neighbors = distances[0:k]\n",
    "    num_good = 0\n",
    "    num_bad = 0\n",
    "    for neighbor in neighbors:\n",
    "        title = neighbor[1]\n",
    "        if labels[title] == 0:\n",
    "            num_bad += 1\n",
    "        elif labels[title] == 1:\n",
    "            num_good += 1\n",
    "    if num_good > num_bad:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Sets\n",
    "\n",
    "To evaluate the K-Nearest Neighbors algorithm, we split the data into training and validation sets. We use the training set to find the k nearest neighbors and make predictions on the validation set. By comparing predictions to actual labels, we calculate the validation accuracy. This helps us assess how well our model performs and choose the best k value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "\n",
    "The value of k in K-Nearest Neighbors affects the model's performance. A small k, like 1, may lead to **overfitting**, where the model relies too much on the training data, being overly influenced by outliers. A large k can cause **underfitting**, where the model ignores important nuances of the training set and performs poorly overall. Therefore, finding the right balance for k is crucial to avoid both overfitting and underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_validation_accuracy(\n",
    "    training_set, training_labels, validation_set, validation_labels, k\n",
    "):\n",
    "    num_correct = 0.0\n",
    "    for title in validation_set:\n",
    "        guess = classify(validation_set[title], training_set, training_labels, k)\n",
    "        if guess == validation_labels[title]:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sklearn` library to implement a K-Nearest Neighbor classifier easily.\n",
    "\n",
    "1. Create a Classifier: \\\n",
    "   Initialize a `KNeighborsClassifier` with a specified k value.\n",
    "2. Train the Classifier: \\\n",
    "   Use the `.fit()` method to train it with your data points and their labels.\n",
    "3. Classify New Points: \\\n",
    "   Use the `.predict()` method to classify new data points based on the trained model.\n",
    "\n",
    "This approach simplifies the process of implementing K-Nearest Neighbors in your projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, labels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(movie_dataset, labels)\n",
    "\n",
    "guess = classifier.predict([[0.45, 0.2, 0.5], [0.25, 0.8, 0.9], [0.1, 0.1, 0.9]])\n",
    "\n",
    "guess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
