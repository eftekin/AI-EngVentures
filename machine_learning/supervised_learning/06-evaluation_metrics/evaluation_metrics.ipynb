{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** is used to evaluate classification models by comparing predicted and actual labels:\n",
    "\n",
    "|                  | **Predicted -**      | **Predicted +**      |\n",
    "|------------------|----------------------|----------------------|\n",
    "| **Actual -**      | True Negative (TN)   | False Positive (FP)   |\n",
    "| **Actual +**      | False Negative (FN)  | True Positive (TP)    |\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "- **Accuracy**: \n",
    "  $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "- **Precision**: \n",
    "  $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "- **Recall**: \n",
    "  $$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "- **F1-Score**: \n",
    "  $$ \\text{F1-Score} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "These metrics help measure the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 3 4\n",
      "[[0 3]\n",
      " [4 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "\n",
    "print(true_positives, true_negatives, false_positives, false_negatives)\n",
    "\n",
    "conf_matrix = confusion_matrix(actual, predicted)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "**Accuracy** is a common metric for evaluating classification models. It is calculated as the ratio of correctly classified predictions (True Positives and True Negatives) to the total number of predictions.\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where:\n",
    "- **TP** = True Positives\n",
    "- **TN** = True Negatives\n",
    "- **FP** = False Positives\n",
    "- **FN** = False Negatives\n",
    "\n",
    "Let's calculate the accuracy of the classification algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (true_positives + true_negatives) / len(predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "**Recall** is useful when the goal is to capture as many true positive cases as possible. It measures the ratio of correct positive predictions (True Positives) to the total number of actual positive cases.\n",
    "\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "Where:\n",
    "- **TP** = True Positives\n",
    "- **FN** = False Negatives\n",
    "\n",
    "Recall is the ratio of correct positive classifications made by the model to all actual positives. For example, in a spam classifier, recall would be the number of correctly labeled spam emails divided by all actual spam emails in the dataset.\n",
    "\n",
    "A model that always predicts \"not spam\" might have high accuracy, but its recall will be 0 because it never identifies any true positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = true_positives/(true_positives + false_negatives)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "**Precision** helps us understand the accuracy of the positive predictions made by the model. It measures the ratio of correct positive predictions (True Positives) to the total number of positive predictions.\n",
    "\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "Where:\n",
    "- **TP** = True Positives\n",
    "- **FP** = False Positives\n",
    "\n",
    "Precision is the ratio of correct positive classifications to all positive classifications made by the model. For example, in a spam classifier, precision would be the number of correctly labeled spam emails divided by all the emails predicted as spam (correct or incorrect).\n",
    "\n",
    "A model that predicts every email is spam would have a recall of 1, but very low precision due to the large number of false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = true_positives/(true_positives + false_positives)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score\n",
    "\n",
    "The **F1-score** combines both precision and recall into a single statistic by calculating their harmonic mean. This is useful because it accounts for both precision and recall in a balanced way, and provides a low score if either precision or recall is low.\n",
    "\n",
    "$$ \\text{F1-score} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "We use the harmonic mean instead of the arithmetic mean because we want the F1-score to be low when either precision or recall is close to 0.\n",
    "\n",
    "For example, if recall = 1 and precision = 0.02:\n",
    "\n",
    "- Arithmetic mean: \n",
    "  $$ \\frac{1 + 0.02}{2} = 0.51 $$\n",
    "  \n",
    "  This value seems high for such a low precision.\n",
    "\n",
    "- Harmonic mean (F1-score): \n",
    "  $$ \\frac{2 \\times 1 \\times 0.02}{1 + 0.02} = 0.039 $$\n",
    "\n",
    "  This result more accurately reflects the effectiveness of the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4615384615384615"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_1 = 2*precision*recall/(precision+recall)\n",
    "f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "There is no perfect metric for evaluating a classification model. The decision to use **accuracy**, **precision**, **recall**, **F1-score**, or another metric depends on the specific context of the problem.\n",
    "\n",
    "For example, in the email spam classification problem:\n",
    "- We may prefer a model with high **precision** to avoid mistakenly labeling important emails as spam, even if it means some spam emails end up in the inbox (low recall).\n",
    "\n",
    "Understanding the question you're trying to answer will guide you in choosing the most relevant statistic for your problem.\n",
    "\n",
    "The Python library **scikit-learn** provides functions to calculate all of these metrics.\n",
    "\n",
    "Key Takeaways:\n",
    "- Classifications can result in **True Positive (TP)**, **True Negative (TN)**, **False Positive (FP)**, or **False Negative (FN)**. These values are summarized in a **confusion matrix**.\n",
    "- **Accuracy** measures the proportion of correct classifications out of all classifications made.\n",
    "- **Recall** is the ratio of correct positive classifications to all actual positives.\n",
    "- **Precision** is the ratio of correct positive classifications to all predicted positives.\n",
    "- **F1-score** combines precision and recall. It will be low if either precision or recall is low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.42857142857142855\n",
      "0.5\n",
      "0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(accuracy_score(actual, predicted))\n",
    "\n",
    "print(recall_score(actual, predicted))\n",
    "\n",
    "print(precision_score(actual, predicted))\n",
    "\n",
    "print(f1_score(actual,predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
