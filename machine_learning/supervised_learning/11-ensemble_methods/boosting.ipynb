{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Boosting Machine Learning Models\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Boosting\n","\n","Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. It focuses on training new models to correct the errors made by existing models, which helps improve performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Adaptive Boosting Overview\n","\n","Adaptive Boosting, or AdaBoost, is one of the first boosting algorithms. It combines multiple weak classifiers (often decision trees) to create a strong classifier.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n","{'algorithm': 'SAMME.R', 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': 1, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__monotonic_cst': None, 'estimator__random_state': None, 'estimator__splitter': 'best', 'estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 5, 'random_state': None}\n","Test set accuracy:\t0.8574181117533719\n","Test set precision:\t0.7247191011235955\n","Test set recall:\t0.8376623376623377\n","Test set f1-score:\t0.7771084337349398\n","Confusion Matrix:\n","            predicted yes  predicted no\n","actual yes            129            25\n","actual no              49           316\n"]},{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n","  warnings.warn(\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    confusion_matrix,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Load dataset to a pandas DataFrame\n","path_to_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n","column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"accep\"]\n","df = pd.read_csv(path_to_data, names=column_names)\n","\n","target_column = \"accep\"\n","raw_feature_columns = [col for col in column_names if col != target_column]\n","\n","# Create dummy variables from the feature columns\n","X = pd.get_dummies(df[raw_feature_columns], drop_first=True)\n","\n","# Convert target column to binary variable; 0 if 'unacc', 1 otherwise\n","df[target_column] = np.where(df[target_column] == \"unacc\", 0, 1)\n","y = df[target_column]\n","\n","# Split the full dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, random_state=123, test_size=0.3\n",")\n","\n","# 1. Create a decision stump base model using the Decision Tree Classifier and print its parameters\n","decision_stump = DecisionTreeClassifier(max_depth=1)\n","print(decision_stump.get_params())\n","\n","# 2. Create an Adaptive Boost Classifier and print its parameters\n","ada_classifier = AdaBoostClassifier(estimator=decision_stump, n_estimators=5)\n","print(ada_classifier.get_params())\n","\n","# 3. Fit the Adaptive Boost Classifier to the training data and get the list of predictions\n","ada_classifier.fit(X_train, y_train)\n","y_pred = ada_classifier.predict(X_test)\n","\n","# 4. Calculate the accuracy, precision, recall, and f1-score on the testing data\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Test set accuracy:\\t{accuracy}\")\n","print(f\"Test set precision:\\t{precision}\")\n","print(f\"Test set recall:\\t{recall}\")\n","print(f\"Test set f1-score:\\t{f1}\")\n","\n","# 5. Print the confusion matrix\n","test_conf_matrix = pd.DataFrame(\n","    confusion_matrix(y_test, y_pred, labels=[1, 0]),\n","    index=[\"actual yes\", \"actual no\"],\n","    columns=[\"predicted yes\", \"predicted no\"],\n",")\n","\n","print(f\"Confusion Matrix:\\n{test_conf_matrix.to_string()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","### Gradient Boosting Overview\n","\n","Gradient Boosting is another powerful boosting technique that builds models in a stage-wise fashion and generalizes them by allowing optimization of an arbitrary differentiable loss function.\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 15, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n","Test set accuracy:\t0.8978805394990366\n","Test set precision:\t0.7885714285714286\n","Test set recall:\t0.8961038961038961\n","Test set f1-score:\t0.8389057750759878\n","Confusion Matrix:\n","            predicted yes  predicted no\n","actual yes            138            16\n","actual no              37           328\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    confusion_matrix,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset to a pandas DataFrame\n","path_to_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n","column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"accep\"]\n","\n","df = pd.read_csv(path_to_data, names=column_names)\n","target_column = \"accep\"\n","raw_feature_columns = [col for col in column_names if col != target_column]\n","\n","# Create dummy variables from the feature columns\n","X = pd.get_dummies(df[raw_feature_columns], drop_first=True)\n","\n","# Convert target column to binary variable; 0 if 'unacc', 1 otherwise\n","df[target_column] = np.where(df[target_column] == \"unacc\", 0, 1)\n","y = df[target_column]\n","\n","# Split the full dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, random_state=123, test_size=0.3\n",")\n","\n","# 1. Create a Gradient Boosting Classifier and print its parameters\n","grad_classifier = GradientBoostingClassifier(n_estimators=15)\n","\n","print(grad_classifier.get_params())\n","\n","# 2. Fit the Gradient Boosted Trees Classifier to the training data and get the list of predictions\n","grad_classifier.fit(X_train, y_train)\n","y_pred = grad_classifier.predict(X_test)\n","\n","# 3. Calculate the accuracy, precision, recall, and f1-score on the testing data\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Test set accuracy:\\t{accuracy}\")\n","print(f\"Test set precision:\\t{precision}\")\n","print(f\"Test set recall:\\t{recall}\")\n","print(f\"Test set f1-score:\\t{f1}\")\n","\n","# 4. Print the confusion matrix\n","test_conf_matrix = pd.DataFrame(\n","    confusion_matrix(y_test, y_pred, labels=[1, 0]),\n","    index=[\"actual yes\", \"actual no\"],\n","    columns=[\"predicted yes\", \"predicted no\"],\n",")\n","\n","print(f\"Confusion Matrix:\\n{test_conf_matrix.to_string()}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
